import torch
from transformers import BertTokenizer, BertModel, pipeline, RobertaTokenizer, RobertaModel

from classification_by_bert.config import Config
from pre_config import PreDatasetConfig

# config = PreDatasetConfig()
# # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# # model = BertForMaskedLM(config.bert_config)
# data = torch.load('./gpu_bert_checkpoints/ckpt_0.pth', map_location='cuda:0')
# # model.load_state_dict(['generator'])
# print()
config = Config(dataset='../dataset', name="Bert4Layers")


def t_tokenizer():
    row_text = '4685 19551 18319 20614 8612 13839 21618 15014 3780 1396 29956 10063 14485 10922 28136 20097 14668 21695 16598 10026 2143 14950 26336 1744 1242 13177 12235 17281 27160 1396 5379 5836 27653 13516 ， 4685 22224 22224 20614 8612 13839 26823 3145 17281 979 20265 20265 13535 10449 20669 6728 14360 17983 9614 17281 18970 18865 ， 7923 1242 17281 13535 27363 26537 17935 6872 979 8842 28329 17281 10300 19057 29731 11295 5310 17281 23904 5697 9043 。 19636 17018 ， 18319 20614 8612 23025 3110 9042 23904 10935 1882 24007 22160 5368 17281 4685 19551 18319 20614 8612 13839 21618 15014 10922 13613 2281 5120 ， 7230 21720 749 9115 8251 8679 21534 4198 ！ 10449 16997 1938 26823 17281 21857 21857 29265 26108 28685 10811 15014 3241 ， 3780 1396 7659 30020 17281 14547 2240 30139 29956 10063 15469 ， 29504 13177 12235 13712 ， 6636 2337 25672 24053 20048 10449 26537 14668 17281 18970 18865 29956 3618 4054 13174 ？ 16867 16867 1258 3144 12153 21618 15014 17281 22275 18754 17861 18970 16542 20983 3644 29095 ， 15299 19912 16868 2166 10647 20614 8612 9810 6494 17281 14547 20097 14668 15469 20823 23404 26823 16648 7442 12052 ， 1116 24007 22160 5368 19121 11160 396 15185 14547 20097 14668 2199 2224 15469 21618 15014 。 2281 15014 3241 9601 ， 13177 12235 16456 25501 10238 20163 4685 19551 18319 20614 8612 13839 21618 15014 7710 29652 20447 17281 20097 14668 2199 2224 9810 18947 17857 4397 1242 21871 7710 20097 14668 2199 2224 11583 15707 10674 19078 。 9019 1390 28874 979 4971 28898 20614 8612 15014 3241 21942 14950 17281 26213 27700 13712 ， 979 2738 3470 22736 4685 19551 18319 20614 8612 13839 21618 15014 3780 1396 29956 10063 14485 10922 28136 20097 14668 21695 16598 10026 2143 14950 26336 1744 1242 13177 12235 17281 27160 1396 5379 5836 27653 13516 ， 4685 22224 22224 20614 8612 13839 26823 3145 17281 979 20265 20265 13535 10449 20669 6728 14360 17983 9614 17281 18970 18865 ， 7923 1242 17281 13535 27363 26537 17935 6872 979 8842 28329 17281 10300 19057 29731 11295 5310 17281 23904 5697 9043 。 19636 17018 ， 18319 20614 8612 23025 3110 9042 23904 10935 1882 24007 22160 5368 17281 4685 19551 18319 20614 8612 13839 21618 15014 10922 13613 2281 5120 ， 7230 21720 749 9115 8251 8679 21534 4198 ！ 10449 16997 1938 26823 17281 21857 21857 29265 26108 28685 10811 15014 3241 ， 3780 1396 7659 30020 17281 14547 2240 30139 29956 10063 15469 ， 29504 13177 12235 13712 ， 6636 2337 25672 24053 20048 10449 26537 14668 17281 18970 18865 29956 3618 4054 13174 ？ 16867 16867 1258 3144 12153 21618 15014 17281 22275 18754 17861 18970 16542 20983 3644 29095 ， 15299 19912 16868 2166 10647 20614 8612 9810 6494 17281 14547 20097 14668 15469 20823 23404 26823 16648 7442 12052 ， 1116 24007 22160 5368 19121 11160 396 15185 14547 20097 14668 2199 2224 15469 21618 15014 。 2281 15014 3241 9601 ， 13177 12235 16456 25501 10238 20163 4685 19551 18319 20614 8612 13839 21618 15014 7710 29652 20447 17281 20097 14668 2199 2224 9810 18947 17857 4397 1242 21871 7710 20097 14668 2199 2224 11583 15707 10674 19078 。 9019 1390 28874 979 4971 28898 20614 8612 15014 3241 21942 14950 17281 26213 27700 13712 ， 979 2738 3470 22736 4685 19551 18319 20614 8612 13839 21618 15014 3780 1396 29956 10063 14485 10922 28136 20097 14668 21695 16598 10026 2143 14950 26336 1744 1242 13177 12235 17281 27160 1396 5379 5836 27653 13516 ， 4685 22224 22224 20614 8612 13839 26823 3145 17281 979 20265 20265 13535 10449 20669 6728 14360 17983 9614 17281 18970 18865 ， 7923 1242 17281 13535 27363 26537 17935 6872 979 8842 28329 17281 10300 19057 29731 11295 5310 17281 23904 5697 9043 。 19636 17018 ， 18319 20614 8612 23025 3110 9042 23904 10935 1882 24007 22160 5368 17281 4685 19551 18319 20614 8612 13839 21618 15014 10922 13613 2281 5120 ， 7230 21720 749 9115 8251 8679 21534 4198 ！ 10449 16997 1938 26823 17281 21857 21857 29265 26108 28685 10811 15014 3241 ， 3780 1396 7659 30020 17281 14547 2240 30139 29956 10063 15469 ， 29504 13177 12235 13712 ， 6636 2337 25672 24053 20048 10449 26537 14668 17281 18970 18865 29956 3618 4054 13174 ？ 16867 16867 1258 3144 12153 21618 15014 17281 22275 18754 17861 18970 16542 20983 3644 29095 ， 15299 19912 16868 2166 10647 20614 8612 9810 6494 17281 14547 20097 14668 15469 20823 23404 26823 16648 7442 12052 ， 1116 24007 22160 5368 19121 11160 396 15185 14547 20097 14668 2199 2224 15469 21618 15014 。 2281 15014 3241 9601 ， 13177 12235 16456 25501 10238 20163 4685 19551 18319 20614 8612 13839 21618 15014 7710 29652 20447 17281 20097 14668 2199 2224 9810 18947 17857 4397 1242 21871 7710 20097 14668 2199 2224 11583 15707 10674 19078 。 9019 1390 28874 979 4971 28898 20614 8612 15014 3241 21942 14950 17281 26213 27700 13712 ， 979 2738 3470 22736 4685 19551 18319 20614 8612 13839 21618 15014 3780 1396 29956 10063 14485 10922 28136 20097 14668 21695 16598 10026 2143 14950 26336 1744 1242 13177 12235 17281 27160 1396 5379 5836 27653 13516 ， 4685 22224 22224 20614 8612 13839 26823 3145 17281 979 20265 20265 13535 10449 20669 6728 14360 17983 9614 17281 18970 18865 ， 7923 1242 17281 13535 27363 26537 17935 6872 979 8842 28329 17281 10300 19057 29731 11295 5310 17281 23904 5697 9043 。 19636 17018 ， 18319 20614 8612 23025 3110 9042 23904 10935 1882 24007 22160 5368 17281 4685 19551 18319 20614 8612 13839 21618 15014 10922 13613 2281 5120 ， 7230 21720 749 9115 8251 8679 21534 4198 ！ 10449 16997 1938 26823 17281 21857 21857 29265 26108 28685 10811 15014 3241 ， 3780 1396 7659 30020 17281 14547 2240 30139 29956 10063 15469 ， 29504 13177 12235 13712 ， 6636 2337 25672 24053 20048 10449 26537 14668 17281 18970 18865 29956 3618 4054 13174 ？ 16867 16867 1258 3144 12153 21618 15014 17281 22275 18754 17861 18970 16542 20983 3644 29095 ， 15299 19912 16868 2166 10647 20614 8612 9810 6494 17281 14547 20097 14668 15469 20823 23404 26823 16648 7442 12052 ， 1116 24007 22160 5368 19121 11160 396 15185 14547 20097 14668 2199 2224 15469 21618 15014 。 2281 15014 3241 9601 ， 13177 12235 16456 25501 10238 20163 4685 19551 18319 20614 8612 13839 21618 15014 7710 29652 20447 17281 20097 14668 2199 2224 9810 18947 17857 4397 1242 21871 7710 20097 14668 2199 2224 11583 15707 10674 19078 。 9019 1390 28874 979 4971 28898 20614 8612 15014 3241 21942 14950 17281 26213 27700 13712 ， 979 2738 3470 22736'
    token = BertTokenizer.from_pretrained("../dataset/vocab.txt")
    # print(len(row_text.split(" ")))
    # print(token(row_text)['input_ids'])
    # print(token.get_vocab())
    print(type(token.vocab_size))


def t_fill_mask():
    model = BertModel.from_pretrained("../dataset/MyBert")
    token = BertTokenizer.from_pretrained("../dataset/vocab.txt")
    fill = pipeline('fill-mask', model="../dataset/MyBert", tokenizer=token)
    row_str = [
        "4685 19551 18319 20614 8612 13839 21618 15014 3780 1396 29956 10063 14485 10922 28136 20097 14668 21695 16598 10026 2143",
        "8496 5697 4925 13516 19636 13462 14950 16542 17482 ， 24965 23904 24748 30020 ， 16572 8612 4873 26568 24058 26669 28136 6619 6619 4054"
    ]
    mask_str = [
        f"4685 19551 18319 20614 8612 13839 {fill.tokenizer.mask_token} 15014 3780 1396 29956 10063 14485 10922 28136 20097 14668 21695 16598 10026 2143",
        f"8496 5697 4925 13516 19636 13462 14950 {fill.tokenizer.mask_token} 17482 ， 24965 23904 24748 30020 ， 16572 8612 4873 26568 24058 26669 28136 6619 6619 4054"
    ]
    mask_token = ['21618', '16542']
    for index, row_text in enumerate(mask_str):
        print("**" * 30)
        results = fill(row_text)
        for res in results:
            print("Sequence is:{}".format(res['sequence']))
            print("predict  token is:{}".format(res['token_str']))
        print("mask token is {}".format(mask_token[index]))


def t_robert_para():
    model = RobertaModel.from_pretrained(config.roberta_local)
    for name, para in model.named_parameters():
        print(name)


if __name__ == '__main__':
    t_robert_para()
